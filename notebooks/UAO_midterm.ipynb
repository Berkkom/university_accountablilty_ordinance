{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ddf2e8",
   "metadata": {},
   "source": [
    "\n",
    "# University Accountability Ordinance (UAO) — Midterm Notebook\n",
    "**Course:** CS506 Data Science (Spring 2025)  \n",
    "**Client:** Councilor Liz Breadon’s Office  \n",
    "**Notebook generated:** 2025-10-27 22:08\n",
    "\n",
    "This notebook is the working artifact for the **midterm deliverables**:\n",
    "- preliminary visualizations (EDA),\n",
    "- detailed data processing description (with executable steps),\n",
    "- modeling methods prototyped so far,\n",
    "- preliminary results.\n",
    "\n",
    "> Tip: Keep cells concise and add short markdown notes above each cell describing why the step is needed and what changed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed109eb",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11e88cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/berkkom/Desktop/university_accountablilty_ordinance/.venv/bin/python\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!which python\n",
    "!!pip install numpy pandas matplotlib scikit-learn --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e02143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geopandas not available (optional).\n",
      "No module named 'geopandas'\n",
      "Project root: /Users/berkkom/Desktop/university_accountablilty_ordinance/notebooks\n",
      "Data directories created (if missing).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== Environment Checks & Imports ====\n",
    "# If a package import fails, install it locally (e.g., `pip install pandas scikit-learn pyproj shapely`).\n",
    "# Keep your environment pinned via requirements.txt for reproducibility.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import pathlib\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Core DS stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Viz (follow course rules: use matplotlib for charts)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modeling\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, classification_report\n",
    "except Exception as e:\n",
    "    print('scikit-learn not available yet. Install it if you plan to run the modeling cells.')\n",
    "    print(e)\n",
    "\n",
    "# GIS helpers (optional, comment out if unavailable)\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "except Exception as e:\n",
    "    print('geopandas not available (optional).')\n",
    "    print(e)\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = pathlib.Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "CLEAN_DIR = DATA_DIR / 'cleaned'\n",
    "VIZ_DIR = PROJECT_ROOT / 'visualizations'\n",
    "\n",
    "for d in [DATA_DIR, RAW_DIR, CLEAN_DIR, VIZ_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Project root:', PROJECT_ROOT)\n",
    "print('Data directories created (if missing).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe17313",
   "metadata": {},
   "source": [
    "## 2. Project Constants & Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28b60151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured local data files:\n",
      " - violations: /Users/berkkom/Desktop/university_accountablilty_ordinance/notebooks/data/raw/violations_2016_2024.csv\n",
      "Configured data URLs (verify resource_ids in your environment).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Official data sources (update if your team keeps copied snapshots in Drive) ===\n",
    "URLS = {\n",
    "    \"violations\": \"https://data.boston.gov/api/3/action/datastore_search?resource_id=800a2663-1d6a-46e7-9356-bedb70f5332c&limit=50000\",\n",
    "    \"requests_311\": \"https://data.boston.gov/api/3/action/datastore_search?resource_id=2968e2c0-d479-49ba-a884-4ef523ada3c0&limit=50000\",  # check resource_id for 311\n",
    "    \"sam_addresses\": \"https://data.boston.gov/api/3/action/datastore_search?resource_id=4d01b43b-49f2-4e56-bc1b-cb7738eae6b2&limit=50000\", # verify latest SAM resource\n",
    "    \"assessments\": \"https://data.boston.gov/api/3/action/datastore_search?resource_id=0625b6ca-7f79-4e0f-bb55-9d4a63f877f8&limit=50000\",   # property assessment example id\n",
    "    # Student Housing Reports (UAO): typically provided via client/drive — place CSVs in data/raw/uao/\n",
    "}\n",
    "\n",
    "DATA_FILES = {\n",
    "    \"violations\": RAW_DIR / \"violations_2016_2024.csv\",\n",
    "    # Add others when ready:\n",
    "    # \"311\": RAW_DIR / \"requests311_2016_2024.csv\",\n",
    "    # \"assessments\": RAW_DIR / \"assessments_2024.csv\",\n",
    "}\n",
    "\n",
    "# Years of interest\n",
    "YEARS = list(range(2016, 2025))\n",
    "print(\"Configured local data files:\")\n",
    "for name, path in DATA_FILES.items():\n",
    "    print(f\" - {name}: {path}\")\n",
    "print('Configured data URLs (verify resource_ids in your environment).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16186418",
   "metadata": {},
   "source": [
    "## 3. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f626c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Downloader utilities (CKAN-style API) ===\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "def fetch_ckan_json(url: str) -> dict:\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as resp:\n",
    "            return json.loads(resp.read().decode('utf-8'))\n",
    "    except urllib.error.URLError as e:\n",
    "        print('Fetch failed:', url, e)\n",
    "        return {}\n",
    "\n",
    "def records_to_df(payload: dict, key: str = 'result') -> pd.DataFrame:\n",
    "    if not payload:\n",
    "        return pd.DataFrame()\n",
    "    result = payload.get(key, {})\n",
    "    recs = result.get('records', [])\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "# Example: pull a small sample to get schema; for full pulls consider paging (offset/limit).\n",
    "violations_sample = records_to_df(fetch_ckan_json(URLS['violations']))\n",
    "print('Violations sample shape:', violations_sample.shape)\n",
    "violations_sample.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130bd92b",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Load Local Snapshots (Preferred for Reproducibility)\n",
    "Place raw CSV extracts in `data/raw/` first (e.g., exported from the portal or provided by the client).\n",
    "Name suggestions (adjust to your files):\n",
    "- `violations_2016_2024.csv`\n",
    "- `requests311_2016_2024.csv`\n",
    "- `assessments_2024.csv`\n",
    "- `sam_addresses.csv`\n",
    "- `uao_students_2016_2024.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e70e089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_DIR: /Users/berkkom/Desktop/university_accountablilty_ordinance/notebooks/data/raw\n",
      "Files in RAW_DIR:\n",
      " - violations_2016_2024.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"RAW_DIR:\", RAW_DIR)\n",
    "print(\"Files in RAW_DIR:\")\n",
    "for p in sorted(RAW_DIR.glob(\"*\")):\n",
    "    print(\" -\", p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8d1445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded violations_2016_2024.csv: (16953, 25)\n",
      "[WARN] Missing file: /Users/berkkom/Desktop/university_accountablilty_ordinance/notebooks/data/raw/requests311_2016_2024.csv\n",
      "[WARN] Missing file: /Users/berkkom/Desktop/university_accountablilty_ordinance/notebooks/data/raw/assessments_2024.csv\n",
      "[WARN] Missing file: /Users/berkkom/Desktop/university_accountablilty_ordinance/notebooks/data/raw/sam_addresses.csv\n",
      "[WARN] Missing file: /Users/berkkom/Desktop/university_accountablilty_ordinance/notebooks/data/raw/uao_students_2016_2024.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16953 entries, 0 to 16952\n",
      "Data columns (total 25 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   _id               16953 non-null  int64  \n",
      " 1   case_no           16953 non-null  object \n",
      " 2   ap_case_defn_key  16953 non-null  int64  \n",
      " 3   status_dttm       16952 non-null  object \n",
      " 4   status            16953 non-null  object \n",
      " 5   code              16953 non-null  object \n",
      " 6   value             0 non-null      float64\n",
      " 7   description       16706 non-null  object \n",
      " 8   violation_stno    16953 non-null  object \n",
      " 9   violation_sthigh  4288 non-null   object \n",
      " 10  violation_street  16953 non-null  object \n",
      " 11  violation_suffix  16809 non-null  object \n",
      " 12  violation_city    16953 non-null  object \n",
      " 13  violation_state   16953 non-null  object \n",
      " 14  violation_zip     16951 non-null  object \n",
      " 15  ward              16953 non-null  object \n",
      " 16  contact_addr1     16947 non-null  object \n",
      " 17  contact_addr2     3039 non-null   object \n",
      " 18  contact_city      16951 non-null  object \n",
      " 19  contact_state     16951 non-null  object \n",
      " 20  contact_zip       16940 non-null  object \n",
      " 21  sam_id            16861 non-null  float64\n",
      " 22  latitude          16860 non-null  float64\n",
      " 23  longitude         16860 non-null  float64\n",
      " 24  location          16860 non-null  object \n",
      "dtypes: float64(4), int64(2), object(19)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Template loaders — update filenames to your actual snapshots\n",
    "def load_csv_safe(path: pathlib.Path, **kwargs) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        print(f'[WARN] Missing file: {path}')\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        df = pd.read_csv(path, **kwargs)\n",
    "        print(f'[OK] Loaded {path.name}:', df.shape)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'[ERR] Could not load {path}:', e)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "df_viol = load_csv_safe(RAW_DIR / 'violations_2016_2024.csv', low_memory=False)\n",
    "df_311  = load_csv_safe(RAW_DIR / 'requests311_2016_2024.csv', low_memory=False)\n",
    "df_assess = load_csv_safe(RAW_DIR / 'assessments_2024.csv', low_memory=False)\n",
    "df_sam = load_csv_safe(RAW_DIR / 'sam_addresses.csv', low_memory=False)\n",
    "df_uao = load_csv_safe(RAW_DIR / 'uao_students_2016_2024.csv', low_memory=False)\n",
    "\n",
    "df_viol.head()\n",
    "df_viol.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff6909f",
   "metadata": {},
   "source": [
    "## 5. Cleaning & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Address normalization helpers\n",
    "import re\n",
    "\n",
    "def normalize_address(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return ''\n",
    "    s = s.upper().strip()\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    # common street suffix normalizations (expand as needed)\n",
    "    s = s.replace(' STREET', ' ST').replace(' AVENUE', ' AVE').replace(' ROAD', ' RD')\n",
    "    s = s.replace(' PLACE', ' PL').replace(' COURT', ' CT').replace(' BOULEVARD', ' BLVD')\n",
    "    return s\n",
    "\n",
    "# Example cleaning pipeline for violations\n",
    "def clean_violations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    out = df.copy()\n",
    "    out.columns = [c.strip().lower() for c in out.columns]\n",
    "\n",
    "    # --- Parse date ---\n",
    "    out[\"status_dttm\"] = pd.to_datetime(out[\"status_dttm\"], errors=\"coerce\")\n",
    "    out[\"year\"] = out[\"status_dttm\"].dt.year\n",
    "\n",
    "    # --- Build a single normalized address string ---\n",
    "    out[\"full_address\"] = (\n",
    "        out[\"violation_stno\"].astype(str).fillna(\"\") + \" \" +\n",
    "        out[\"violation_street\"].astype(str).fillna(\"\") + \" \" +\n",
    "        out[\"violation_suffix\"].astype(str).fillna(\"\") + \", \" +\n",
    "        out[\"violation_city\"].astype(str).fillna(\"\") + \", \" +\n",
    "        out[\"violation_state\"].astype(str).fillna(\"\") + \" \" +\n",
    "        out[\"violation_zip\"].astype(str).fillna(\"\")\n",
    "    )\n",
    "    out[\"full_address\"] = out[\"full_address\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip().str.upper()\n",
    "\n",
    "    # --- Add a simplified severity label ---\n",
    "    def classify_severity(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"other\"\n",
    "        text = text.upper()\n",
    "        if any(k in text for k in [\"SMOKE\", \"STRUCT\", \"ELECTRICAL\", \"FIRE\", \"CO\", \"HEAT\", \"HAZARD\"]):\n",
    "            return \"severe\"\n",
    "        elif any(k in text for k in [\"SANIT\", \"PEST\", \"TRASH\", \"PLUMBING\"]):\n",
    "            return \"moderate\"\n",
    "        else:\n",
    "            return \"minor\"\n",
    "\n",
    "    out[\"severity\"] = out[\"description\"].map(classify_severity)\n",
    "\n",
    "    # --- Drop duplicates and irrelevant columns (optional) ---\n",
    "    out = out.drop_duplicates(subset=[\"case_no\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "def clean_311(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    out.columns = [c.strip().lower().replace(' ', '_') for c in out.columns]\n",
    "    for c in [col for col in ['open_dt','closed_dt','target_dt','on_time_dt'] if col in out.columns]:\n",
    "        out[c] = pd.to_datetime(out[c], errors='coerce')\n",
    "    # Filter to relevant categories; adjust to match your schema\n",
    "    if 'subject' in out.columns:\n",
    "        mask = out['subject'].str.contains('HOUS|SANIT|NOISE|HEAT|WATER', case=False, na=False)\n",
    "        out = out[mask]\n",
    "    return out\n",
    "\n",
    "def clean_assess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    out.columns = [c.strip().lower().replace(' ', '_') for c in out.columns]\n",
    "    # Numeric conversions\n",
    "    for c in ['total_value','land_value','building_value','year_built','living_area']:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors='coerce')\n",
    "    return out\n",
    "\n",
    "df_viol_c = clean_violations(df_viol)\n",
    "df_311_c  = clean_311(df_311)\n",
    "df_assess_c = clean_assess(df_assess)\n",
    "\n",
    "for name, df in [('violations', df_viol_c), ('311', df_311_c), ('assess', df_assess_c)]:\n",
    "    print(name, df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a36181",
   "metadata": {},
   "source": [
    "## 6. Linking Datasets via Address / SAM ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce325c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Expectation: SAM dataset includes a stable address identifier (e.g., 'SAM_ID' or similar).\n",
    "# Adjust these keys to your actual column names.\n",
    "\n",
    "def prepare_sam(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    out.columns = [c.strip().lower().replace(' ', '_') for c in out.columns]\n",
    "    # Example expected columns; update for your schema:\n",
    "    # 'sam_id', 'full_address', 'x', 'y', 'latitude', 'longitude'\n",
    "    if 'full_address' in out.columns:\n",
    "        out['full_address_norm'] = out['full_address'].map(normalize_address)\n",
    "    return out\n",
    "\n",
    "df_sam_c = prepare_sam(df_sam)\n",
    "\n",
    "# Example: join violations to SAM by normalized address\n",
    "def link_to_sam(df_any: pd.DataFrame, df_sam_c: pd.DataFrame, address_col_guess: List[str]) -> pd.DataFrame:\n",
    "    if df_any.empty or df_sam_c.empty:\n",
    "        return df_any\n",
    "    any_c = df_any.copy()\n",
    "    for col in address_col_guess:\n",
    "        if col in any_c.columns:\n",
    "            any_c[col + '_norm'] = any_c[col].map(normalize_address)\n",
    "            # left join on normalized address\n",
    "            any_c = any_c.merge(\n",
    "                df_sam_c[['full_address_norm','sam_id']] if 'sam_id' in df_sam_c.columns else df_sam_c[['full_address_norm']],\n",
    "                left_on=col + '_norm',\n",
    "                right_on='full_address_norm',\n",
    "                how='left'\n",
    "            )\n",
    "            break\n",
    "    return any_c\n",
    "\n",
    "df_viol_linked = link_to_sam(df_viol_c, df_sam_c, address_col_guess=['address','full_address','location'])\n",
    "df_311_linked  = link_to_sam(df_311_c, df_sam_c, address_col_guess=['address','full_address','location'])\n",
    "\n",
    "print('Linked violations shape:', df_viol_linked.shape)\n",
    "print('Linked 311 shape:', df_311_linked.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc460636",
   "metadata": {},
   "source": [
    "## 7. Student Presence Flags (UAO merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cbb5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Expectation: df_uao has columns like ['address','year','student_count'] per address/year.\n",
    "# Adjust to your schema and join key.\n",
    "\n",
    "def prepare_uao(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    out.columns = [c.strip().lower().replace(' ', '_') for c in out.columns]\n",
    "    if 'address' in out.columns:\n",
    "        out['address_norm'] = out['address'].map(normalize_address)\n",
    "    return out\n",
    "\n",
    "df_uao_c = prepare_uao(df_uao)\n",
    "\n",
    "def tag_student_addresses(df_any: pd.DataFrame, df_uao_c: pd.DataFrame, by_year=True) -> pd.DataFrame:\n",
    "    if df_any.empty or df_uao_c.empty:\n",
    "        return df_any\n",
    "    out = df_any.copy()\n",
    "    # join key: normalized address (+ year if available)\n",
    "    if by_year and 'year' in out.columns and 'year' in df_uao_c.columns:\n",
    "        out = out.merge(\n",
    "            df_uao_c[['address_norm','year','student_count']],\n",
    "            left_on=['address_norm','year'] if 'address_norm' in out.columns else ['address_norm','year'],  # adjust if necessary\n",
    "            right_on=['address_norm','year'],\n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        out = out.merge(\n",
    "            df_uao_c[['address_norm','student_count']],\n",
    "            on='address_norm',\n",
    "            how='left'\n",
    "        )\n",
    "    out['is_student_address'] = out['student_count'].fillna(0) > 0\n",
    "    return out\n",
    "\n",
    "# Apply only if your cleaned frames have 'address_norm'\n",
    "if 'address_norm' in df_viol_linked.columns:\n",
    "    df_viol_tagged = tag_student_addresses(df_viol_linked, df_uao_c)\n",
    "else:\n",
    "    df_viol_tagged = df_viol_linked\n",
    "\n",
    "print('Student-tagged violations shape:', df_viol_tagged.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a04629",
   "metadata": {},
   "source": [
    "## 8. Preliminary Visualizations (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.1 Violation counts by year (student-linked vs others)\n",
    "if not df_viol_tagged.empty and 'year' in df_viol_tagged.columns:\n",
    "    vc = (\n",
    "        df_viol_tagged\n",
    "        .assign(student=lambda d: np.where(d['is_student_address'], 'student', 'non-student') if 'is_student_address' in d.columns else 'unknown')\n",
    "        .groupby(['year','student'])\n",
    "        .size()\n",
    "        .reset_index(name='count')\n",
    "        .pivot(index='year', columns='student', values='count')\n",
    "        .fillna(0)\n",
    "    )\n",
    "    ax = vc.plot(kind='line', marker='o', figsize=(8,5))  # matplotlib is used under the hood\n",
    "    ax.set_title('Violation Counts by Year (Student vs Non-Student)')\n",
    "    ax.set_xlabel('Year'); ax.set_ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Skip 8.1: df_viol_tagged empty or missing year.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d60820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.2 Top violation types among student-linked properties\n",
    "if not df_viol_tagged.empty:\n",
    "    # Guess a violation description column; update as needed\n",
    "    viol_col = 'viol_desc' if 'viol_desc' in df_viol_tagged.columns else None\n",
    "    if viol_col:\n",
    "        top = (\n",
    "            df_viol_tagged[df_viol_tagged.get('is_student_address', False) == True]\n",
    "            .groupby(viol_col)\n",
    "            .size()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(15)\n",
    "        )\n",
    "        ax = top.plot(kind='bar', figsize=(9,5))\n",
    "        ax.set_title('Top Violation Types (Student-Linked)')\n",
    "        ax.set_xlabel('Violation'); ax.set_ylabel('Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Skip 8.2: violation description column not found.')\n",
    "else:\n",
    "    print('Skip 8.2: df_viol_tagged empty.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8484929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.3 Property value distributions: student vs non-student (join assess + student tags by address/SAM)\n",
    "if not df_assess_c.empty and not df_viol_tagged.empty:\n",
    "    # Build a list of known student addresses from UAO\n",
    "    student_addrs = set(df_uao_c['address_norm']) if not df_uao_c.empty and 'address_norm' in df_uao_c.columns else set()\n",
    "    df_assess_c['address_norm'] = df_assess_c.get('address', '').map(lambda x: x.upper().strip() if isinstance(x,str) else '')\n",
    "    df_assess_c['is_student_address'] = df_assess_c['address_norm'].isin(student_addrs)\n",
    "    # Choose a value column\n",
    "    val_col = 'total_value' if 'total_value' in df_assess_c.columns else None\n",
    "    if val_col:\n",
    "        grouped = df_assess_c.groupby('is_student_address')[val_col].describe()\n",
    "        print(grouped)\n",
    "        # Simple boxplot\n",
    "        ax = df_assess_c.boxplot(column=val_col, by='is_student_address', figsize=(7,5))\n",
    "        plt.title('Property Values by Student Address Flag'); plt.suptitle('')\n",
    "        plt.xlabel('Is Student Address'); plt.ylabel('Assessed Value')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Skip 8.3: total_value column not found in assessments.')\n",
    "else:\n",
    "    print('Skip 8.3: missing assess or violations data.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d8865",
   "metadata": {},
   "source": [
    "## 9. Modeling Prototype — Predicting Non-Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc62ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define non-compliance label: >=3 violations within a 24-month window (example)\n",
    "def label_non_compliance(df: pd.DataFrame, window_days: int = 730) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    out = df.copy()\n",
    "    # Ensure date exists\n",
    "    date_col = None\n",
    "    for c in ['date','open_dt','issued_date','status_date']:\n",
    "        if c in out.columns:\n",
    "            date_col = c; break\n",
    "    if not date_col:\n",
    "        print('No date column to compute windows.')\n",
    "        out['non_compliant'] = np.nan\n",
    "        return out\n",
    "    out = out.sort_values([date_col])\n",
    "    # Group by address (or SAM ID if available)\n",
    "    key = 'sam_id' if 'sam_id' in out.columns else ('address_norm' if 'address_norm' in out.columns else None)\n",
    "    if not key:\n",
    "        print('No join key (sam_id/address_norm); labeling by global sequence.')\n",
    "        key = None\n",
    "    labels = []\n",
    "    if key:\n",
    "        for addr, grp in out.groupby(key):\n",
    "            dates = grp[date_col].dropna().sort_values().values\n",
    "            # Sliding window over sorted dates\n",
    "            count_label = 0\n",
    "            if len(dates) >= 3:\n",
    "                i = 0\n",
    "                for j in range(len(dates)):\n",
    "                    while dates[j] - dates[i] > np.timedelta64(window_days, 'D'):\n",
    "                        i += 1\n",
    "                    if (j - i + 1) >= 3:\n",
    "                        count_label = 1\n",
    "                        break\n",
    "            labels.extend([count_label]*len(grp))\n",
    "    else:\n",
    "        labels = [0]*len(out)\n",
    "    out['non_compliant'] = labels if labels else 0\n",
    "    return out\n",
    "\n",
    "df_for_model = label_non_compliance(df_viol_tagged)\n",
    "\n",
    "# Feature engineering (very light; extend for final)\n",
    "def build_features(df_viol: pd.DataFrame, df_311: pd.DataFrame, df_assess: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_viol.empty:\n",
    "        return pd.DataFrame()\n",
    "    key = 'sam_id' if 'sam_id' in df_viol.columns else ('address_norm' if 'address_norm' in df_viol.columns else None)\n",
    "    if not key:\n",
    "        print('No key for features.')\n",
    "        return pd.DataFrame()\n",
    "    # Violations per address\n",
    "    f_viol = df_viol.groupby(key).agg(\n",
    "        viol_count=('non_compliant', 'size'),\n",
    "        severe_count=('severity', lambda s: (s=='severe').sum() if 'severe' in s.values or True else 0),\n",
    "        non_compliant_any=('non_compliant', 'max')\n",
    "    ).reset_index()\n",
    "    # 311 per address (optional)\n",
    "    if not df_311.empty:\n",
    "        if key in df_311.columns:\n",
    "            f_311 = df_311.groupby(key).size().reset_index(name='req311_count')\n",
    "            f_viol = f_viol.merge(f_311, on=key, how='left')\n",
    "        else:\n",
    "            f_viol['req311_count'] = 0\n",
    "    else:\n",
    "        f_viol['req311_count'] = 0\n",
    "    # Assessment merge\n",
    "    if not df_assess.empty and key in df_assess.columns:\n",
    "        cols = [c for c in ['total_value','year_built','living_area'] if c in df_assess.columns]\n",
    "        f_viol = f_viol.merge(df_assess[[key] + cols], on=key, how='left')\n",
    "    # Fill NaNs\n",
    "    f_viol = f_viol.fillna({'req311_count': 0})\n",
    "    return f_viol\n",
    "\n",
    "features = build_features(df_for_model, df_311_linked, df_assess_c)\n",
    "print('Feature table shape:', features.shape)\n",
    "features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a simple logistic regression (if sklearn is available and enough data)\n",
    "try:\n",
    "    target = 'non_compliant_any'\n",
    "    feature_cols = [c for c in ['viol_count','severe_count','req311_count','total_value','year_built','living_area'] if c in features.columns]\n",
    "    if len(feature_cols) >= 2 and target in features.columns:\n",
    "        model_df = features.dropna(subset=feature_cols + [target]).copy()\n",
    "        X = model_df[feature_cols].values\n",
    "        y = model_df[target].values.astype(int)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(X_train)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "        clf = LogisticRegression(max_iter=200)\n",
    "        clf.fit(X_train_s, y_train)\n",
    "        y_proba = clf.predict_proba(X_test_s)[:,1]\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        print('Logistic Regression AUROC:', round(auc, 3))\n",
    "    else:\n",
    "        print('Not enough feature columns or missing target; skip modeling.')\n",
    "except Exception as e:\n",
    "    print('Modeling step skipped due to error:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3cad7",
   "metadata": {},
   "source": [
    "## 10. Preliminary Results (Fill in as you run cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c6578",
   "metadata": {},
   "source": [
    "\n",
    "- **Coverage:** XX% of violation records matched to SAM / addresses.\n",
    "- **Top violation categories:** (list from Section 8.2)\n",
    "- **Temporal trend:** (brief 1–2 sentence insight from 8.1)\n",
    "- **Value comparison:** (insight from 8.3 boxplot)\n",
    "- **Modeling:** Logistic regression AUROC = `X.XX` (features: ...). Notes on what improves/harms performance.\n",
    "\n",
    "> Save figures to `visualizations/` and reference them in your README midterm report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2117a",
   "metadata": {},
   "source": [
    "## 11. Utility: Save Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_current_fig(name: str, directory: pathlib.Path = VIZ_DIR):\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    outpath = directory / name\n",
    "    plt.savefig(outpath, bbox_inches='tight', dpi=180)\n",
    "    print('Saved:', outpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4724e",
   "metadata": {},
   "source": [
    "## 12. Appendix — Reproducibility Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae73e9d",
   "metadata": {},
   "source": [
    "\n",
    "- Keep raw data immutable in `data/raw/`.  \n",
    "- Output cleaned tables to `data/cleaned/` with versioned filenames.  \n",
    "- Track your environment in `requirements.txt` (pin to minor versions).  \n",
    "- Document any manual data fixes in a `CHANGELOG.md` with rationale and links.  \n",
    "- Consider adding a `Makefile` or `invoke` tasks to orchestrate end-to-end runs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
